{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jannik/miniconda3/lib/python3.10/site-packages/jax/_src/api_util.py:190: SyntaxWarning: Jitted function has static_argnums=(0, 1, 9), but only accepts 8 positional arguments. This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has {argnums_name}={argnums}, \"\n"
     ]
    }
   ],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from models import VisionTransformer\n",
    "\n",
    "from clu import metrics\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from flax import struct                # Flax dataclasses\n",
    "import optax                           # Common loss functions and optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizon = 10 \n",
    "# dt = 0.1 \n",
    "# rec_dt = 0.025  # 40 Hz data collection\n",
    "# every_n = int(dt/rec_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# import torch\n",
    "\n",
    "# class FieldDataset(Dataset):\n",
    "#     def __init__(self, file_dir='data/robot_field_data.pt'):\n",
    "#         grids, commands = torch.load(file_dir)\n",
    "        \n",
    "#         self.grids = grids[::every_n].to(torch.float32)\n",
    "#         self.commands = commands[::every_n][:,[0,1,5]].to(torch.float32)  # get u_x, u_y, u_w\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.grids) - horizon\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         grid = self.grids[idx]\n",
    "#         commands = self.commands[idx:idx+horizon]\n",
    "#         return grid, commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 252\n",
      "Length of validation dataset: 64\n"
     ]
    }
   ],
   "source": [
    "# dataset = FieldDataset('data/impossible_obstacle.pt')\n",
    "\n",
    "# train_ratio = 0.8\n",
    "\n",
    "# train_size = int((train_ratio) * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# print(f'Length of training dataset: {len(train_dataset)}')\n",
    "# print(f'Length of validation dataset: {len(val_dataset)}')\n",
    "\n",
    "# batch_size = 1\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     'patches': (4,4),\n",
    "#     'transformer': {\n",
    "#       'num_layers': 3,\n",
    "#       'mlp_dim': 64,\n",
    "#       'num_heads': 1,\n",
    "#       'dropout_rate': 0.1,\n",
    "#       'attention_dropout_rate': 0.1,\n",
    "#     },\n",
    "#     'solver': {\n",
    "#       'dt': dt,\n",
    "#       'horizon': horizon,\n",
    "#     },\n",
    "#     'hidden_size': 42,\n",
    "#     'num_output': 6*6+6,\n",
    "#     'representation_size': 64,\n",
    "# }\n",
    "\n",
    "\n",
    "# model = VisionTransformer(**config)\n",
    "\n",
    "# print(model.tabulate({'params': jax.random.PRNGKey(0), 'dropout': jax.random.PRNGKey(1)}, \n",
    "#                      jnp.ones((1, 28, 28, 1)), \n",
    "#                      train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @struct.dataclass\n",
    "# class Metrics(metrics.Collection):\n",
    "#   # accuracy: metrics.Accuracy\n",
    "#   loss: metrics.Average.from_output('loss')\n",
    "  \n",
    "# class TrainState(train_state.TrainState):\n",
    "#   metrics: Metrics\n",
    "\n",
    "# def create_train_state(module, rng, learning_rate, momentum, train=True):\n",
    "#   \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  \n",
    "#   # initialize parameters by passing a template image\n",
    "#   params = module.init(rng, jnp.ones([1, 100, 100, 1]), train=train)['params'] \n",
    "  \n",
    "#   tx = optax.sgd(learning_rate, momentum)\n",
    "#   return TrainState.create(\n",
    "#       apply_fn=module.apply, params=params, tx=tx,\n",
    "#       metrics=Metrics.empty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "# def train_step(state, batch, train=True):\n",
    "#   \"\"\"Train for a single step.\"\"\"\n",
    "#   def loss_fn(params):\n",
    "#     dropout_rng = jax.random.PRNGKey(1)\n",
    "    \n",
    "#     pred = state.apply_fn({'params': params},\n",
    "#                             rngs={'dropout': dropout_rng},\n",
    "#                             inputs=batch['inputs'], \n",
    "#                             train=train)\n",
    "        \n",
    "#     loss = optax.l2_loss(pred, batch['targets']).mean()\n",
    "#     return loss\n",
    "  \n",
    "#   grad_fn = jax.grad(loss_fn)\n",
    "#   grads = grad_fn(state.params)\n",
    "  \n",
    "#   state = state.apply_gradients(grads=grads)\n",
    "#   return state\n",
    "\n",
    "# @jax.jit\n",
    "# def compute_metrics(*, state, batch, train=True):\n",
    "#   dropout_rng = jax.random.PRNGKey(1)\n",
    "  \n",
    "#   pred = state.apply_fn({'params': state.params},\n",
    "#                           rngs={'dropout': dropout_rng},\n",
    "#                           inputs=batch['inputs'], \n",
    "#                           train=train)\n",
    "  \n",
    "#   loss = optax.l2_loss(pred, batch['targets']).mean()\n",
    "  \n",
    "#   metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "#   metrics = state.metrics.merge(metric_updates)\n",
    "  \n",
    "#   state = state.replace(metrics=metrics)\n",
    "#   return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.3654899  -0.0301541  -0.01419155]\n",
      "  [ 0.36607835 -0.02743974 -0.01207794]\n",
      "  [ 0.36695573 -0.02452096 -0.01001463]\n",
      "  [ 0.36812302 -0.02139261 -0.00800748]\n",
      "  [ 0.36958176 -0.01804929 -0.00606247]\n",
      "  [ 0.371334   -0.01448542 -0.0041857 ]\n",
      "  [ 0.37338242 -0.01069519 -0.0023834 ]\n",
      "  [ 0.37573037 -0.00667263 -0.00066193]\n",
      "  [ 0.37838173 -0.00241157  0.00097221]\n",
      "  [ 0.38134107  0.00209433  0.0025124 ]]]\n"
     ]
    }
   ],
   "source": [
    "# grid, _ = dataset.__getitem__(100)\n",
    "# input = jnp.expand_dims(grid.numpy(), (0,-1))\n",
    "\n",
    "# init_rng = {'params': jax.random.PRNGKey(0), 'dropout': jax.random.PRNGKey(1)}\n",
    "\n",
    "# params = model.init(init_rng, jnp.ones([1, 100, 100, 1]), train=True)['params'] \n",
    "\n",
    "# output = model.apply({'params': params}, input, rngs=init_rng, train=True)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_rng = {'params': jax.random.PRNGKey(0), 'dropout': jax.random.PRNGKey(1)}\n",
    "\n",
    "train = True\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "\n",
    "state = create_train_state(model, init_rng, learning_rate, momentum)\n",
    "del init_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:40<00:00,  6.29it/s, loss=0.013564912] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 0, loss: 0.013564911670982838, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 111/252 [00:12<00:16,  8.76it/s, loss=0.012433872] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m grids \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnumpy\u001b[39m.\u001b[39mexpand_dims(grids\u001b[39m.\u001b[39mnumpy(), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m batch \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m: grids, \u001b[39m'\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m'\u001b[39m: commands\u001b[39m.\u001b[39mnumpy()}\n\u001b[0;32m---> 14\u001b[0m state \u001b[39m=\u001b[39m train_step(state, batch) \u001b[39m# get updated train state (which contains the updated parameters)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m state \u001b[39m=\u001b[39m compute_metrics(state\u001b[39m=\u001b[39mstate, batch\u001b[39m=\u001b[39mbatch) \u001b[39m# aggregate batch metrics\u001b[39;00m\n\u001b[1;32m     17\u001b[0m data_iterator\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39mstate\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mcompute_value()\u001b[39m.\u001b[39mvalue)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/flax/core/frozen_dict.py:162\u001b[0m, in \u001b[0;36mFrozenDict.tree_unflatten\u001b[0;34m(cls, keys, values)\u001b[0m\n\u001b[1;32m    157\u001b[0m   sorted_keys \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dict)\n\u001b[1;32m    158\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m    159\u001b[0m       [(jax\u001b[39m.\u001b[39mtree_util\u001b[39m.\u001b[39mDictKey(k), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dict[k]) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m sorted_keys]\n\u001b[1;32m    160\u001b[0m   ), \u001b[39mtuple\u001b[39m(sorted_keys)\n\u001b[0;32m--> 162\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtree_unflatten\u001b[39m(\u001b[39mcls\u001b[39m, keys, values):\n\u001b[1;32m    164\u001b[0m   \u001b[39m# data is already deep copied due to tree map mechanism\u001b[39;00m\n\u001b[1;32m    165\u001b[0m   \u001b[39m# we can skip the deep copy in the constructor\u001b[39;00m\n\u001b[1;32m    166\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m({k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(keys, values)}, __unsafe_skip_copy__\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "metrics_history = {'train_loss': []}\n",
    "num_epochs = 10\n",
    "epoch_iterator = range(num_epochs)\n",
    "\n",
    "for epoch in epoch_iterator:\n",
    "  data_iterator = tqdm(train_loader)\n",
    "  for grids, commands in data_iterator:\n",
    "    grids = jax.numpy.expand_dims(grids.numpy(), axis=-1)\n",
    "    \n",
    "    batch = {'inputs': grids, 'targets': commands.numpy()}\n",
    "    \n",
    "    state = train_step(state, batch) # get updated train state (which contains the updated parameters)\n",
    "    state = compute_metrics(state=state, batch=batch) # aggregate batch metrics\n",
    "    \n",
    "    data_iterator.set_postfix(loss=state.metrics.loss.compute_value().value)\n",
    "  \n",
    "  for metric,value in state.metrics.compute().items():  # compute metrics\n",
    "    metrics_history[f'train_{metric}'].append(value)    # record metrics\n",
    "  state = state.replace(metrics=state.metrics.empty()) \n",
    "  \n",
    "  print(f\"train epoch: {epoch}, \"\n",
    "        f\"loss: {metrics_history['train_loss'][-1]}, \")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
