{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "\n",
    "import einops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import casadi as ca\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, patch_size=5):\n",
    "    \"\"\"Splitting images into patches.\n",
    "    Args:\n",
    "        images: Input tensor with size (batch, channels, height, width)\n",
    "    Returns:\n",
    "        A batch of image patches with size (\n",
    "          batch, (height / patch_size) * (width / patch_size), \n",
    "        channels * patch_size * patch_size)\n",
    "    \"\"\"\n",
    "    return einops.rearrange(\n",
    "        images,\n",
    "        'b (h p1) (w p2) -> b (h w) (p1 p2)',\n",
    "        p1=patch_size,\n",
    "        p2=patch_size\n",
    "    )\n",
    "\n",
    "def unpatchify(patches, patch_size=5):\n",
    "    \"\"\"Combining patches into images.\n",
    "    Args:\n",
    "        patches: Input tensor with size (\n",
    "        batch, (height / patch_size) * (width / patch_size), \n",
    "        channels * patch_size * patch_size)\n",
    "    Returns:\n",
    "        A batch of images with size (batch, channels, height, width)\n",
    "    \"\"\"\n",
    "    return einops.rearrange(\n",
    "        patches,\n",
    "        'b (h w) (p1 p2) -> b (h p1) (w p2)',\n",
    "        p1=patch_size,\n",
    "        p2=patch_size,\n",
    "        h=int(patches.shape[1] ** 0.5),\n",
    "        w=int(patches.shape[1] ** 0.5),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids, commands = torch.load('data/robot_field_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 25])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_patchified = patchify([grids[0]])\n",
    "grid_patchified.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer Encoder \n",
    "    Args:\n",
    "        embedding_dim: dimension of embedding\n",
    "        n_heads: number of attention heads\n",
    "        n_layers: number of attention layers\n",
    "        feedforward_dim: hidden dimension of MLP layer\n",
    "    Returns:\n",
    "        Transformer embedding of input\n",
    "    \"\"\"\n",
    "    # TODO embedding_dim? -> size of Q,p ???\n",
    "    def __init__(self, embedding_dim=256, n_heads=1, n_layers=3, feedforward_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=self.n_heads,\n",
    "                dim_feedforward=self.feedforward_dim,\n",
    "                activation=F.gelu,\n",
    "                batch_first=True,\n",
    "                dropout=0.1,\n",
    "            ),\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimization Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_N=20\n",
    "opt_dt=0.1\n",
    "\n",
    "opti = ca.Opti()\n",
    "\n",
    "opt_x=opti.variable(3, opt_N+1)\n",
    "opt_u=opti.variable(3, opt_N)\n",
    "\n",
    "opt_x0 = opti.parameter(3)\n",
    "opt_u0 = opti.parameter(3)\n",
    "\n",
    "opt_u_des = opti.parameter(3)\n",
    "\n",
    "opt_P = opti.parameter(6, 6)\n",
    "opt_q = opti.parameter(6)\n",
    "\n",
    "# stage cost\n",
    "cost = 0\n",
    "for i in range(opt_N):\n",
    "  cost += 0.1*(ca.vertcat(opt_x[:, i], opt_u[:, i]).T @ opt_P.T @ opt_P @ ca.vertcat(opt_x[:, i], opt_u[:, i])\n",
    "                   + opt_q.T @ ca.vertcat(opt_x[:, i], opt_u[:, i]))\n",
    "  cost += 10*(opt_u_des - opt_u[:, i]).T @ (opt_u_des - opt_u[:, i])\n",
    "  \n",
    "opti.minimize(cost)\n",
    "\n",
    "# system dynamics\n",
    "for i in range(opt_N):\n",
    "  opti.subject_to(opt_x[0, i+1] == opt_x[0, i] + opt_dt * (ca.cos(opt_x[2, i]) * opt_u[0, i] - ca.sin(opt_x[2, i]) * opt_u[1, i] + opt_u[2, i]) )\n",
    "  opti.subject_to(opt_x[1, i+1] == opt_x[1, i] + opt_dt * (ca.sin(opt_x[2, i]) * opt_u[0, i] + ca.cos(opt_x[2, i]) * opt_u[1, i] + opt_u[2, i]))\n",
    "  opti.subject_to(opt_x[2, i+1] == opt_x[2, i] + opt_dt * (opt_u[2, i]))\n",
    "  \n",
    "  # constraints on control rate\n",
    "  # for i in range(N):\n",
    "  #   opti.subject_to( u[:,i+1] - u[:,i] <= np.array([0.1, 0.1, 0.1]) )\n",
    "  #   opti.subject_to( u[:,i+1] - u[:,i] >= -np.array([0.1, 0.1, 0.1]) )\n",
    "  # opti.subject_to( u[:,0] - u0 <= np.array([0.1, 0.1, 0.1]) )\n",
    "  # opti.subject_to( u[:,0] - u0 >= -np.array([0.1, 0.1, 0.1]) )\n",
    "  \n",
    "  # initial condition\n",
    "  opti.subject_to(opt_x[:, 0] == opt_x0)\n",
    "  \n",
    "opti.solver('ipopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFTOC(Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, x0, u0, P, q, u_des):\n",
    "    \n",
    "    u_opt = []\n",
    "    x_opt = []\n",
    "    \n",
    "    # loop through batch\n",
    "    for i in range(P.shape[0]):\n",
    "      opti.set_value(opt_x0, x0)\n",
    "      opti.set_value(opt_u0, u0)\n",
    "    \n",
    "      opti.set_value(opt_u_des, u_des)\n",
    "\n",
    "      opti.set_value(opt_P, P[i].detach().numpy())\n",
    "      opti.set_value(opt_q, q[i].detach().numpy())\n",
    "\n",
    "      sol = opti.solve()\n",
    "\n",
    "      u_opt.append(sol.value(opt_u))        \n",
    "      x_opt.append(sol.value(opt_x))        \n",
    "    \n",
    "    u_opt = torch.Tensor(u_opt)\n",
    "    x_opt = torch.Tensor(x_opt)\n",
    "    \n",
    "    ctx.save_for_backward(u_opt)\n",
    "    \n",
    "    return x_opt, u_opt\n",
    "    \n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "      result, = ctx.saved_tensors\n",
    "      return grad_output * result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Ipopt version 3.14.11, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:      360\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:      420\n",
      "\n",
      "Total number of variables............................:      123\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:      120\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  3.2000000e+01 0.00e+00 8.00e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  2.7733391e-31 1.11e-16 2.22e-15  -1.0 8.00e-01    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 1\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   2.7733391199176196e-31    2.7733391199176196e-31\n",
      "Dual infeasibility......:   2.2204460492503332e-15    2.2204460492503332e-15\n",
      "Constraint violation....:   1.1102230246251565e-16    1.1102230246251565e-16\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Overall NLP error.......:   2.2204460492503332e-15    2.2204460492503332e-15\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 2\n",
      "Number of objective gradient evaluations             = 2\n",
      "Number of equality constraint evaluations            = 2\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 2\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 1\n",
      "Total seconds in IPOPT                               = 0.009\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n",
      "       nlp_f  |   2.46ms ( 94.58us)   2.62ms (100.87us)        26\n",
      "       nlp_g  |   2.80ms (107.88us)   2.81ms (107.91us)        26\n",
      "    nlp_grad  |   5.18ms (575.44us)   5.53ms (614.63us)         9\n",
      "  nlp_grad_f  |   8.06ms (223.78us)   8.43ms (234.04us)        36\n",
      "  nlp_hess_l  |  10.97ms (685.56us)  11.86ms (741.53us)        16\n",
      "   nlp_jac_g  |   8.42ms (233.94us)   8.92ms (247.87us)        36\n",
      "       total  |   9.82ms (  9.82ms)  10.54ms ( 10.54ms)         1\n",
      "This is Ipopt version 3.14.11, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:      360\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:      420\n",
      "\n",
      "Total number of variables............................:      123\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:      120\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  3.2000000e+01 0.00e+00 8.00e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  2.7733391e-31 1.11e-16 2.22e-15  -1.0 8.00e-01    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 1\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   2.7733391199176196e-31    2.7733391199176196e-31\n",
      "Dual infeasibility......:   2.2204460492503332e-15    2.2204460492503332e-15\n",
      "Constraint violation....:   1.1102230246251565e-16    1.1102230246251565e-16\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Overall NLP error.......:   2.2204460492503332e-15    2.2204460492503332e-15\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 2\n",
      "Number of objective gradient evaluations             = 2\n",
      "Number of equality constraint evaluations            = 2\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 2\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 1\n",
      "Total seconds in IPOPT                               = 0.008\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n",
      "       nlp_f  |   2.60ms ( 92.75us)   2.77ms ( 98.98us)        28\n",
      "       nlp_g  |   2.94ms (105.14us)   2.94ms (104.92us)        28\n",
      "    nlp_grad  |   5.54ms (554.20us)   5.93ms (592.70us)        10\n",
      "  nlp_grad_f  |   8.56ms (219.59us)   8.97ms (230.11us)        39\n",
      "  nlp_hess_l  |  11.57ms (680.53us)  12.52ms (736.30us)        17\n",
      "   nlp_jac_g  |   8.93ms (228.87us)   9.47ms (242.88us)        39\n",
      "       total  |   8.85ms (  8.85ms)   9.44ms (  9.44ms)         1\n",
      "tensor([[[ 4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,\n",
      "           4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,\n",
      "           4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,\n",
      "           4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01],\n",
      "         [ 1.1438e-29,  1.0847e-29,  1.1636e-29,  1.0650e-29,  1.1143e-29,\n",
      "           9.8608e-30,  8.4803e-30,  9.0719e-30,  9.2691e-30,  8.6775e-30,\n",
      "           7.9872e-30,  7.0997e-30,  3.9443e-31,  3.9443e-30,  3.0075e-30,\n",
      "           2.3666e-30,  4.2401e-30,  0.0000e+00, -3.2171e-30,  0.0000e+00],\n",
      "         [ 5.1276e-30,  5.7192e-30,  6.1137e-30,  5.9165e-30,  6.9025e-30,\n",
      "           4.3387e-30,  5.3248e-30,  3.3527e-30,  4.3387e-30,  5.7192e-30,\n",
      "           3.3527e-30,  3.9443e-30,  1.1833e-30, -1.1833e-30,  4.6099e-30,\n",
      "          -7.8886e-31,  3.3527e-30, -1.5777e-30, -9.6142e-31,  0.0000e+00]],\n",
      "\n",
      "        [[ 4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,\n",
      "           4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,\n",
      "           4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,\n",
      "           4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01,  4.0000e-01],\n",
      "         [ 1.1438e-29,  1.0847e-29,  1.1636e-29,  1.0650e-29,  1.1143e-29,\n",
      "           9.8608e-30,  8.4803e-30,  9.0719e-30,  9.2691e-30,  8.6775e-30,\n",
      "           7.9872e-30,  7.0997e-30,  3.9443e-31,  3.9443e-30,  3.0075e-30,\n",
      "           2.3666e-30,  4.2401e-30,  0.0000e+00, -3.2171e-30,  0.0000e+00],\n",
      "         [ 5.1276e-30,  5.7192e-30,  6.1137e-30,  5.9165e-30,  6.9025e-30,\n",
      "           4.3387e-30,  5.3248e-30,  3.3527e-30,  4.3387e-30,  5.7192e-30,\n",
      "           3.3527e-30,  3.9443e-30,  1.1833e-30, -1.1833e-30,  4.6099e-30,\n",
      "          -7.8886e-31,  3.3527e-30, -1.5777e-30, -9.6142e-31,  0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "# Use it by calling the apply method:\n",
    "_, u_opt = CFTOC.apply(np.array([0, 0, 0]), \n",
    "                       np.array([0, 0, 0]), \n",
    "                       torch.zeros((2,6,6)), \n",
    "                       torch.zeros((2,6)), \n",
    "                       np.array([0.4, 0, 0]))\n",
    "\n",
    "print(u_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCTransformer(nn.Module):\n",
    "    \"\"\"MPC transformer\n",
    "    Args:\n",
    "        TODO\n",
    "        embedding_dim: dimension of embedding\n",
    "        patch_size: image patch size\n",
    "        num_patches: number of image patches\n",
    "    Returns:\n",
    "        TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=256, patch_size=5, num_patches=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.transformer = Transformer(embedding_dim)\n",
    "        \n",
    "        self.position_encoding = nn.Parameter(\n",
    "            torch.randn(1, num_patches * num_patches, embedding_dim) * 0.02\n",
    "        )\n",
    "        \n",
    "        self.patch_projection = nn.Linear(patch_size * patch_size, embedding_dim)\n",
    "        \n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim), \n",
    "            nn.Linear(embedding_dim, 6*6+6)  # TODO P,q\n",
    "        )\n",
    "        \n",
    "        self.cftoc = CFTOC()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\" \n",
    "        (1) Splitting images into fixed-size patches; \n",
    "        (2) Linearly embed each image patch, prepend CLS token; \n",
    "        (3) Add position embeddings;\n",
    "        (4) Feed the resulting sequence of vectors to Transformer encoder.\n",
    "        (5) Extract the embeddings corresponding to the CLS token.\n",
    "        (6) Apply output head to the embeddings to obtain the logits\n",
    "        \"\"\"\n",
    "        patches = patchify(images, self.patch_size)\n",
    "        \n",
    "        patch_embeddings = self.patch_projection(patches)\n",
    "        \n",
    "        embeddings = patch_embeddings + self.position_encoding\n",
    "        \n",
    "        transformer_embeddings = self.transformer(embeddings)\n",
    "        transformer_embeddings = transformer_embeddings[:, 0, :]\n",
    "        \n",
    "        output = self.output_head(transformer_embeddings)\n",
    "        \n",
    "        # split and reshape to get P and q for CFTOC cost function\n",
    "        P, q = torch.split(output, [36, 6], dim=1)\n",
    "        P = P.view(P.shape[0], 6, 6)\n",
    "        \n",
    "        \n",
    "        x0 = np.array([0, 0, 0])\n",
    "        u0 = np.array([0, 0, 0])\n",
    "        u_des = np.array([0.4, 0, 0])\n",
    "        \n",
    "        P = P\n",
    "        q = q\n",
    "\n",
    "        _, u_opt = CFTOC.apply(x0, u0, P, q, u_des)\n",
    "\n",
    "        return u_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Ipopt version 3.14.11, running with linear solver MUMPS 5.4.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:      360\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:      420\n",
      "\n",
      "Total number of variables............................:      123\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:      120\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  3.2000000e+01 0.00e+00 7.98e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  2.0139326e+00 1.54e-03 1.11e-01  -1.0 7.11e-01    -  1.00e+00 1.00e+00f  1\n",
      "   2  1.9850177e+00 2.20e-07 1.50e-05  -2.5 2.59e-02    -  1.00e+00 1.00e+00h  1\n",
      "   3  1.9850094e+00 4.52e-14 9.92e-13  -8.6 2.96e-06    -  1.00e+00 1.00e+00h  1\n",
      "\n",
      "Number of Iterations....: 3\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   1.9850093639994026e+00    1.9850093639994026e+00\n",
      "Dual infeasibility......:   9.9220631710750240e-13    9.9220631710750240e-13\n",
      "Constraint violation....:   4.5186077102243871e-14    4.5186077102243871e-14\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Overall NLP error.......:   9.9220631710750240e-13    9.9220631710750240e-13\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 4\n",
      "Number of objective gradient evaluations             = 4\n",
      "Number of equality constraint evaluations            = 4\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 4\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 3\n",
      "Total seconds in IPOPT                               = 0.012\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "      solver  :   t_proc      (avg)   t_wall      (avg)    n_eval\n",
      "       nlp_f  |   2.90ms ( 90.62us)   3.10ms ( 96.89us)        32\n",
      "       nlp_g  |   3.30ms (103.28us)   3.31ms (103.42us)        32\n",
      "    nlp_grad  |   6.09ms (553.36us)   6.52ms (592.78us)        11\n",
      "  nlp_grad_f  |  73.81ms (  1.68ms)   9.68ms (220.10us)        44\n",
      "  nlp_hess_l  |  13.45ms (672.50us)  14.53ms (726.55us)        20\n",
      "   nlp_jac_g  |   9.58ms (217.70us)  10.18ms (231.36us)        44\n",
      "       total  | 141.90ms (141.90ms)  13.76ms ( 13.76ms)         1\n",
      "tensor([[[ 0.3707,  0.3710,  0.3713,  0.3716,  0.3720,  0.3724,  0.3728,\n",
      "           0.3733,  0.3737,  0.3743,  0.3748,  0.3754,  0.3760,  0.3767,\n",
      "           0.3774,  0.3782,  0.3789,  0.3798,  0.3806,  0.3816],\n",
      "         [-0.0147, -0.0143, -0.0139, -0.0135, -0.0131, -0.0126, -0.0120,\n",
      "          -0.0114, -0.0108, -0.0101, -0.0094, -0.0086, -0.0078, -0.0069,\n",
      "          -0.0060, -0.0050, -0.0040, -0.0030, -0.0019, -0.0007],\n",
      "         [-0.0496, -0.0469, -0.0442, -0.0415, -0.0386, -0.0357, -0.0328,\n",
      "          -0.0297, -0.0266, -0.0233, -0.0201, -0.0167, -0.0132, -0.0097,\n",
      "          -0.0061, -0.0023,  0.0015,  0.0054,  0.0094,  0.0135]]],\n",
      "       grad_fn=<CFTOCBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_652709/3470494248.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_tensor = torch.tensor(grids[0], dtype=torch.float32).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "model = MPCTransformer()\n",
    "\n",
    "test_tensor = torch.tensor(grids[0], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "print(model(test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
