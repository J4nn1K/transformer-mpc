{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Function\n",
    "from torch.autograd.functional import hessian, jacobian\n",
    "\n",
    "import einops\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import casadi as ca\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, patch_size=5):\n",
    "    \"\"\"Splitting images into patches.\n",
    "    Args:\n",
    "        images: Input tensor with size (batch, channels, height, width)\n",
    "    Returns:\n",
    "        A batch of image patches with size (\n",
    "          batch, (height / patch_size) * (width / patch_size), \n",
    "        channels * patch_size * patch_size)\n",
    "    \"\"\"\n",
    "    return einops.rearrange(\n",
    "        images,\n",
    "        'b (h p1) (w p2) -> b (h w) (p1 p2)',\n",
    "        p1=patch_size,\n",
    "        p2=patch_size\n",
    "    )\n",
    "\n",
    "def unpatchify(patches, patch_size=5):\n",
    "    \"\"\"Combining patches into images.\n",
    "    Args:\n",
    "        patches: Input tensor with size (\n",
    "        batch, (height / patch_size) * (width / patch_size), \n",
    "        channels * patch_size * patch_size)\n",
    "    Returns:\n",
    "        A batch of images with size (batch, channels, height, width)\n",
    "    \"\"\"\n",
    "    return einops.rearrange(\n",
    "        patches,\n",
    "        'b (h w) (p1 p2) -> b (h p1) (w p2)',\n",
    "        p1=patch_size,\n",
    "        p2=patch_size,\n",
    "        h=int(patches.shape[1] ** 0.5),\n",
    "        w=int(patches.shape[1] ** 0.5),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids, commands = torch.load('data/robot_field_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400, 25])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_patchified = patchify([grids[0]])\n",
    "grid_patchified.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer Encoder \n",
    "    Args:\n",
    "        embedding_dim: dimension of embedding\n",
    "        n_heads: number of attention heads\n",
    "        n_layers: number of attention layers\n",
    "        feedforward_dim: hidden dimension of MLP layer\n",
    "    Returns:\n",
    "        Transformer embedding of input\n",
    "    \"\"\"\n",
    "    # TODO embedding_dim? -> size of Q,p ???\n",
    "    def __init__(self, embedding_dim=256, n_heads=1, n_layers=3, feedforward_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.feedforward_dim = feedforward_dim\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_dim,\n",
    "                nhead=self.n_heads,\n",
    "                dim_feedforward=self.feedforward_dim,\n",
    "                activation=F.gelu,\n",
    "                batch_first=True,\n",
    "                dropout=0.1,\n",
    "            ),\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimization Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it by calling the apply method:\n",
    "x0 = torch.Tensor([0, 0, 0])\n",
    "x0.requires_grad=True\n",
    "u0 = torch.Tensor([0, 0, 0])\n",
    "u0.requires_grad=True\n",
    "u_des = torch.Tensor([0.4, 0, 0])\n",
    "u_des.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_N=20\n",
    "opt_dt=0.1\n",
    "\n",
    "opti = ca.Opti()\n",
    "\n",
    "opt_x=opti.variable(3, opt_N+1)\n",
    "opt_u=opti.variable(3, opt_N)\n",
    "\n",
    "opt_x0 = opti.parameter(3)\n",
    "opt_u0 = opti.parameter(3)\n",
    "\n",
    "opt_u_des = opti.parameter(3)\n",
    "\n",
    "opt_P = opti.parameter(6, 6)\n",
    "opt_q = opti.parameter(6)\n",
    "\n",
    "# stage cost\n",
    "cost = 0\n",
    "for i in range(opt_N):\n",
    "  cost += 0.1*(ca.vertcat(opt_x[:, i], opt_u[:, i]).T @ opt_P.T @ opt_P @ ca.vertcat(opt_x[:, i], opt_u[:, i])\n",
    "                   + opt_q.T @ ca.vertcat(opt_x[:, i], opt_u[:, i]))\n",
    "  cost += 10*(opt_u_des - opt_u[:, i]).T @ (opt_u_des - opt_u[:, i])\n",
    "  \n",
    "opti.minimize(cost)\n",
    "\n",
    "# system dynamics\n",
    "for i in range(opt_N):\n",
    "  opti.subject_to(opt_x[0, i+1] == opt_x[0, i] + opt_dt * (ca.cos(opt_x[2, i]) * opt_u[0, i] - ca.sin(opt_x[2, i]) * opt_u[1, i] + opt_u[2, i]) )\n",
    "  opti.subject_to(opt_x[1, i+1] == opt_x[1, i] + opt_dt * (ca.sin(opt_x[2, i]) * opt_u[0, i] + ca.cos(opt_x[2, i]) * opt_u[1, i] + opt_u[2, i]))\n",
    "  opti.subject_to(opt_x[2, i+1] == opt_x[2, i] + opt_dt * (opt_u[2, i]))\n",
    "  \n",
    "  # constraints on control rate\n",
    "  # for i in range(N):\n",
    "  #   opti.subject_to( u[:,i+1] - u[:,i] <= np.array([0.1, 0.1, 0.1]) )\n",
    "  #   opti.subject_to( u[:,i+1] - u[:,i] >= -np.array([0.1, 0.1, 0.1]) )\n",
    "  # opti.subject_to( u[:,0] - u0 <= np.array([0.1, 0.1, 0.1]) )\n",
    "  # opti.subject_to( u[:,0] - u0 >= -np.array([0.1, 0.1, 0.1]) )\n",
    "  \n",
    "  # initial condition\n",
    "  opti.subject_to(opt_x[:, 0] == opt_x0)\n",
    "  \n",
    "opti.solver('ipopt', {'ipopt.print_level':0, 'print_time':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFTOC(Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, embedding, x0, u0, u_des):     \n",
    "    # split and reshape to get P and q for CFTOC cost function\n",
    "    P, q = torch.split(embedding, [36, 6], dim=1)\n",
    "    P = P.view(P.shape[0], 6, 6)\n",
    "    batch_dim = P.shape[0]\n",
    "    \n",
    "    u_opt = []\n",
    "    x_opt = []\n",
    "    \n",
    "    # loop through batch\n",
    "    for i in range(batch_dim):\n",
    "      opti.set_value(opt_x0, x0.detach().numpy())\n",
    "      opti.set_value(opt_u0, u0.detach().numpy())\n",
    "    \n",
    "      opti.set_value(opt_u_des, u_des.detach().numpy())\n",
    "\n",
    "      opti.set_value(opt_P, P[i].detach().numpy())\n",
    "      opti.set_value(opt_q, q[i].detach().numpy())\n",
    "\n",
    "      sol = opti.solve()\n",
    "\n",
    "      u_opt.append(sol.value(opt_u))        \n",
    "      x_opt.append(sol.value(opt_x))        \n",
    "    \n",
    "    # u_opt = torch.Tensor(u_opt).flatten(start_dim=1)\n",
    "    \n",
    "    u_opt = torch.Tensor(u_opt).T.flatten(end_dim=1).T\n",
    "    \n",
    "    \n",
    "    # print(u_opt.shape)\n",
    "    # x_opt = torch.Tensor(x_opt).flatten()\n",
    "    \n",
    "    # TODO make devectoring of embedding form P, q part of the cost function to make it\n",
    "    # differentiable towards output\n",
    "        \n",
    "    ctx.save_for_backward(embedding, u_opt, u_des)\n",
    "    \n",
    "    return u_opt\n",
    "  \n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    '''Eq (11) from paper.'''\n",
    "    \n",
    "    # print('DEBUG Upstream gradient:', grad_output.shape)\n",
    "\n",
    "    embedding, u_opt, u_des = ctx.saved_tensors\n",
    "\n",
    "    # Ensure that the tensors have requires_grad set to True\n",
    "    assert embedding.requires_grad, \"Embedding tensor does not require grad\"\n",
    "    assert u_opt.requires_grad, \"u_opt tensor does not require grad\"\n",
    "\n",
    "    batch_dim = embedding.shape[0]\n",
    "\n",
    "    def cost(embedding, u, u_des):        \n",
    "        P, q = torch.split(embedding, [36, 6], dim=0)\n",
    "        P = P.reshape(6, 6)\n",
    "        \n",
    "        print(P, q)\n",
    "\n",
    "        x = torch.zeros(3*(opt_N+1))\n",
    "        x[0:3] = x0.T \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        i = 0\n",
    "        x[3*(i+1)+0] = x[3*i+0] + opt_dt * (torch.cos(x0[2]) * u[3*i] - torch.sin(x0[2]) * u[3*i+1] + u[3*i+2])\n",
    "        x[3+(i+1)+1] = x[3*i+1] + opt_dt * (torch.sin(x0[2]) * u[3*i] + torch.cos(x0[2]) * u[3*i+1] + u[3*i+2])\n",
    "        x[3*(i+1)+2] = x0[2] + opt_dt * (u[3*i+2])  \n",
    "        \n",
    "        i = 1\n",
    "        x[3*(i+1)+0] = x[3*i+0] + opt_dt * (torch.cos(x0[2] + opt_dt * (u[3*i+2])) * u[3*i] - torch.sin(x0[2] + u[i] + opt_dt * (u[3*i+2])) * u[3*i+1] + u[3*i+2])\n",
    "        x[3+(i+1)+1] = x[3*i+1] + opt_dt * (torch.sin(x0[2] + opt_dt * (u[3*i+2])) * u[3*i] + torch.cos(x0[2] + u[i] + opt_dt * (u[3*i+2])) * u[3*i+1] + u[3*i+2])\n",
    "        x[3*(i+1)+2] = x[3*i+2] + opt_dt * (u[3*i+2])      \n",
    "        \n",
    "        i = 2\n",
    "        x[3*(i+1)+0] = x[3*i+0] + opt_dt * (torch.cos(x0[2] + opt_dt * (u[3*(i-1)+2]) + opt_dt * (u[3*i+2])) * u[3*i] - torch.sin(x0[2] + u[i] + opt_dt * (u[3*i+2])) * u[3*i+1] + u[3*i+2])\n",
    "        x[3+(i+1)+1] = x[3*i+1] + opt_dt * (torch.sin(x0[2] + opt_dt * (u[3*(i-1)+2]) + opt_dt * (u[3*i+2])) * u[3*i] + torch.cos(x0[2] + u[i] + opt_dt * (u[3*i+2])) * u[3*i+1] + u[3*i+2])\n",
    "        x[3*(i+1)+2] = x[3*i+2] + opt_dt * (u[3*i+2])  \n",
    "        \n",
    "        \n",
    "        # torch.sum(u[0:3*i+2::3])\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        # for i in range(opt_N):\n",
    "        #   x[3*(i+1)+0] = x[3*i+0] # + opt_dt * (torch.cos(x[3*i+2]) * u[3*i] - torch.sin(x[3*i+2]) * u[3*i+1] + u[3*i+2])\n",
    "        #   x[3+(i+1)+1] = x[3*i+1] # + opt_dt * (torch.sin(x[3*i+2]) * u[3*i] + torch.cos(x[3*i+2]) * u[3*i+1] + u[3*i+2])\n",
    "        #   x[3*(i+1)+2] = x[3*i+2] # + opt_dt * (u[3*i+2])\n",
    "\n",
    "        cost = 0\n",
    "        for i in range(opt_N):\n",
    "            cost += 0.1 * (torch.cat([x[3*i:3*i+3], u[3*i:3*i+3]], dim=-1).T @ P.T\n",
    "                           @ P @ torch.cat([x[3*i:3*i+3], u[3*i:3*i+3]], dim=-1)\n",
    "                           + q.T @ torch.cat([x[3*i:3*i+3], u[3*i:3*i+3]], dim=-1))\n",
    "            cost += 10 * (u_des - u[3*i:3*i+3]).T @ (u_des - u[3*i:3*i+3])       \n",
    "        \n",
    "        # for i in range(opt_N):\n",
    "        #     x[:, i+1] = x[:, i] + opt_dt * (torch.cos(x[2, i]) * u[0, i] - torch.sin(x[2, i]) * u[1, i] + u[2, i])\n",
    "\n",
    "        # cost = 0\n",
    "        # for i in range(opt_N):\n",
    "        #     cost += 0.1 * (torch.cat([x[:, i], u[:, i]], dim=-1).T @ P.T\n",
    "        #                    @ P @ torch.cat([x[:, i], u[:, i]], dim=-1)\n",
    "        #                    + q.T @ torch.cat([x[:, i], u[:, i]], dim=-1))\n",
    "        #     cost += 10 * (u_des - u[:, i]).T @ (u_des - u[:, i])\n",
    "\n",
    "        # cost = u[:-1].T@u[:-1]*torch.cos(x.T@x)\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    grad_embedding_list = []\n",
    "    \n",
    "    for i in range(batch_dim):\n",
    "\n",
    "      cost_hessian = hessian(lambda u: cost(embedding[i], u, u_des), u_opt[i])\n",
    "      cost_hessian_inv = torch.inverse(cost_hessian)\n",
    "\n",
    "      \n",
    "      print(cost_hessian_inv.shape)\n",
    "\n",
    "      # grad_embedding_list.append(grad_embedding_i.unsqueeze(0))\n",
    "\n",
    "    # grad_embedding = torch.cat(grad_embedding_list, dim=0)\n",
    "\n",
    "    return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding = torch.ones((1,42), requires_grad=True)\n",
    "# u_opt = CFTOC.apply(embedding, x0, u0, u_des)\n",
    "# print(u_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCTransformer(nn.Module):\n",
    "    \"\"\"MPC transformer\n",
    "    Args:\n",
    "        TODO\n",
    "        embedding_dim: dimension of embedding\n",
    "        patch_size: image patch size\n",
    "        num_patches: number of image patches\n",
    "    Returns:\n",
    "        TODO\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim=256, patch_size=5, num_patches=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.transformer = Transformer(embedding_dim)\n",
    "        \n",
    "        self.position_encoding = nn.Parameter(\n",
    "            torch.randn(1, num_patches * num_patches, embedding_dim) * 0.02\n",
    "        )\n",
    "        \n",
    "        self.patch_projection = nn.Linear(patch_size * patch_size, embedding_dim)\n",
    "        \n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim), \n",
    "            nn.Linear(embedding_dim, 6*6+6)  # TODO P,q\n",
    "        )\n",
    "        \n",
    "        self.cftoc = CFTOC()\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\" \n",
    "        (1) Splitting images into fixed-size patches; \n",
    "        (2) Linearly embed each image patch, prepend CLS token; \n",
    "        (3) Add position embeddings;\n",
    "        (4) Feed the resulting sequence of vectors to Transformer encoder.\n",
    "        (5) Extract the embeddings corresponding to the CLS token.\n",
    "        (6) Apply output head to the embeddings to obtain the logits\n",
    "        \"\"\"\n",
    "        patches = patchify(images, self.patch_size)\n",
    "        \n",
    "        patch_embeddings = self.patch_projection(patches)\n",
    "        \n",
    "        embeddings = patch_embeddings + self.position_encoding\n",
    "        \n",
    "        transformer_embeddings = self.transformer(embeddings)\n",
    "        transformer_embeddings = transformer_embeddings[:, 0, :]\n",
    "        \n",
    "        output_embeddings = self.output_head(transformer_embeddings)\n",
    "        \n",
    "        u_opt = CFTOC.apply(output_embeddings, x0, u0, u_des)\n",
    "\n",
    "        return u_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jannik/miniconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 60])) that is different to the input size (torch.Size([60])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3270, -0.8240,  0.4814, -0.3080, -0.1347, -0.1071],\n",
      "        [-0.3478, -0.2319, -0.6237,  0.4094, -0.1325,  0.7680],\n",
      "        [ 0.3273, -1.1287,  0.0561,  0.1408,  0.2156,  0.6102],\n",
      "        [-0.9771,  0.7120, -0.0619, -0.2835,  0.1496,  0.0249],\n",
      "        [ 1.0286,  0.1238, -0.4658, -0.0342,  0.3236,  0.2138],\n",
      "        [-0.5620, -0.1156,  0.0703,  0.0972, -0.3066, -0.5429]],\n",
      "       grad_fn=<ReshapeAliasBackward0>) tensor([ 0.3416, -0.9555,  0.2199, -0.1279,  0.1443,  0.3250],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "torch.Size([60, 60])\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "model = MPCTransformer()\n",
    "\n",
    "idx = 1000\n",
    "\n",
    "input_grid = grids[idx].to(torch.float32).unsqueeze(0)\n",
    "input_grid.requires_grad=True\n",
    "\n",
    "# get expert control input\n",
    "u_expert = commands[idx:idx+opt_N].T[[0,1,5],:].unsqueeze(0).flatten()\n",
    "\n",
    "u_model = model(input_grid)\n",
    "\n",
    "# print(u_model.shape)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "loss = loss_func(u_expert, u_model)\n",
    "# print(loss)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n",
      "torch.Size([10])\n",
      "tensor([ 1,  6,  2,  7,  3,  8,  4,  9,  5, 10])\n"
     ]
    }
   ],
   "source": [
    "test = torch.zeros(1,3,20)\n",
    "test = torch.tensor([[1,2,3,4,5],[6,7,8,9,10]])\n",
    "\n",
    "print(test)\n",
    "print((test.T.flatten(end_dim=1)).T.shape)\n",
    "print(test.T.flatten(end_dim=1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3,  4,  5]],\n",
       "\n",
       "        [[ 6,  7,  8,  9, 10]]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
